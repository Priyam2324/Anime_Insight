import os
import requests
import json
import re
import warnings
import subprocess
import sys
import time
from collections import Counter

# --- Flask and Web App Imports ---
from flask import Flask, request, jsonify
from flask_cors import CORS

# --- Your Original Imports ---
from bs4 import BeautifulSoup
import spacy
from transformers import pipeline
import torch

# Suppress all UserWarnings from libraries like transformers
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# --- 1. SETUP & CONFIGURATION ---
# ==============================================================================

app = Flask(__name__)
# Allow CORS for frontends (like Netlify or GitHub Pages)
CORS(app)

# --- Configuration for Deployment ---
# Load secrets from Hugging Face Space Secrets
MAL_CLIENT_ID = os.getenv("MAL_CLIENT_ID")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not MAL_CLIENT_ID or not GOOGLE_API_KEY:
    # This check ensures the app fails immediately if secrets are not configured.
    print("FATAL ERROR: Required secrets (MAL_CLIENT_ID, GOOGLE_API_KEY) are not set in the Hugging Face Space settings.")
    sys.exit(1)

API_BASE_URL = "https://api.myanimelist.net/v2"
# Note: 'mean' is the official MAL rating, which we now use for the score.
FIELDS = "id,title,main_picture,alternative_titles,mean,synopsis,genres"


BASE_STOP_THEMES = {
    'anime', 'show', 'series', 'movie', 'episode', 'manga', 'viewer',
    'reviewer', 'opinion', 'thing', 'lot', 'everything', 'nothing',
    'something', 'anything', 'people', 'person', 'guy', 'man', 'woman',
    'boy', 'girl', 'kid', 'fan', 'author', 'studio'
}

# --- Pre-load Models (Done once on startup) ---
print("Loading local AI models, please wait...")
try:
    try:
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        print("SpaCy model 'en_core_web_sm' not found. Downloading...")
        subprocess.run([sys.executable, "-m", "spacy", "download", "en_core_web_sm"], check=True, capture_output=True)
        nlp = spacy.load("en_core_web_sm")

    # The sentiment model is technically not used for the score anymore, 
    # but kept here if any internal logic changes later, though it is not strictly needed.
    # I will remove it to clean up the dependency list.
    
    print("Local models loaded successfully.")
except Exception as e:
    print(f"\nFatal error: Could not load local models. Exiting. Error: {e}")
    sys.exit(1)

# ==============================================================================
# --- 2. BACKEND LOGIC ---
# ==============================================================================

def calculate_relevance_score(node, query):
    score = 0
    query_words = set(re.split(r'\s|-', query.lower()))
    if not query_words: return 0
    english_title = node.get('alternative_titles', {}).get('en', '').lower()
    japanese_title = node.get('title', '').lower()
    if query.lower() in english_title: score += 100
    for word in query_words:
        if word in english_title: score += 2
        if word in japanese_title: score += 1
    return score

def get_media_from_mal(query):
    all_results = {}
    headers = {'X-MAL-CLIENT-ID': MAL_CLIENT_ID}
    try:
        initial_search_url = f"{API_BASE_URL}/anime?q={requests.utils.quote(query)}&limit=10&fields={FIELDS}"
        response = requests.get(initial_search_url, headers=headers)
        response.raise_for_status()
        initial_data = response.json().get('data', [])
        if not initial_data: return []
        for item in initial_data:
            all_results[item['node']['id']] = {"node": item['node'], "type": "anime"}
        best_initial_match = max(initial_data, key=lambda item: calculate_relevance_score(item['node'], query), default=None)
        if best_initial_match:
            japanese_title_query = best_initial_match['node'].get('title', '')
            if japanese_title_query and japanese_title_query.lower() != query.lower():
                refined_anime_url = f"{API_BASE_URL}/anime?q={requests.utils.quote(japanese_title_query)}&limit=10&fields={FIELDS}"
                refined_manga_url = f"{API_BASE_URL}/manga?q={requests.utils.quote(japanese_title_query)}&limit=10&fields={FIELDS}"
                for url, media_type in [(refined_anime_url, "anime"), (refined_manga_url, "manga")]:
                    data = requests.get(url, headers=headers).json().get('data', [])
                    for item in data:
                        all_results[item['node']['id']] = {"node": item['node'], "type": media_type}
    except requests.exceptions.RequestException as e:
        return {'error': f"API Error: Failed to fetch results from MyAnimeList. {e}"}
    scored_results = [
        {**item, 'relevance_score': calculate_relevance_score(item['node'], query)}
        for item in all_results.values() if item['node'].get('mean', 0) > 0
    ]
    return sorted(scored_results, key=lambda x: x['relevance_score'], reverse=True)

def scrape_reviews_with_requests(reviews_url, max_reviews=50):
    sorted_reviews_url = f"{reviews_url}?s=helpfulness_DESC"
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'})
    reviews = set()
    page_number = 1
    try:
        while len(reviews) < max_reviews:
            reviews_before_scrape = len(reviews)
            response = session.get(f"{sorted_reviews_url}&p={page_number}", timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            review_elements = soup.select("div.review-element")
            if not review_elements: break
            for element in review_elements:
                review_text_div = element.select_one("div.text")
                if review_text_div:
                    for spoiler in review_text_div.select('span.spoiler'): spoiler.replace_with('[SPOILER REMOVED]')
                    for read_more in review_text_div.select("span.js-hidden"): read_more.replace_with(read_more.get_text())
                    reviews.add(review_text_div.get_text(strip=True))
            if len(reviews) == reviews_before_scrape: break
            page_number += 1
        return list(reviews)[:max_reviews]
    except requests.exceptions.RequestException as e:
        return {'error': f"Scraping Error: Failed to scrape reviews. {e}"}

def generate_dynamic_stop_themes(media_node):
    stop_themes = BASE_STOP_THEMES.copy()
    titles = [media_node.get('title', '')]
    titles.append(media_node.get('alternative_titles', {}).get('en', ''))
    titles.extend(media_node.get('alternative_titles', {}).get('synonyms', []))
    for title in titles:
        for word in re.split(r'[\s:-]+', title):
            cleaned = word.lower().strip()
            if cleaned and len(cleaned) > 2: stop_themes.add(cleaned)
    return stop_themes

def extract_all_themes(text, nlp_model, stop_themes):
    doc = nlp_model(text)
    themes = []
    for chunk in doc.noun_chunks:
        theme = chunk.text.lower()
        if len(theme.split()) > 1 and theme not in stop_themes: themes.append(theme)
    for token in doc:
        theme = token.lemma_.lower()
        if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop and len(theme) > 3 and theme not in stop_themes: themes.append(theme)
    return themes


def get_llm_summary(theme_data, mal_score):
    data_section = ""
    for theme, sentences in theme_data.items():
        data_section += f"\nTheme: \"{theme}\"\nRelevant User Comments:\n"
        for sentence in sentences: data_section += f"- \"{sentence}\"\n"
    json_schema = {"type": "object", "properties": {"summary_paragraph": {"type": "string"}}, "required": ["summary_paragraph"]}
    
    prompt = (
        f"You are an expert review analyst. The show's official MyAnimeList rating is **{mal_score}**. "
        "Your task is to write a balanced, professional summary that reflects this score, using the key themes and user comments provided below.\n\n"
        "**CRITICAL INSTRUCTIONS:**\n"
        "1. **Write a Balanced Summary:** Analyze the comments for each theme to identify both strengths and weaknesses. A good summary justifies the score by discussing both. For example, if the score is high (like 8/10), the summary should be mostly positive but still briefly mention what prevented it from being a 10/10.\n"
        "2. **Be Spoiler-Free:** The summary must be **spoiler-free**. Do not reveal major plot points, twists, or character fates.\n"
        "3. **Synthesize, Don't List:** Weave the themes into a fluent, well-written paragraph. Do not just list the themes or repeat the user comments verbatim.\n"
        "4. **Do Not Mention the Score:** Your summary's tone should reflect the score, but **do not mention the numerical score** in your written paragraph.\n\n"
        "Your response must be a single, valid JSON object.\n\n"
        f"--- DATA ---\n{data_section}"
    )
    try:
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={GOOGLE_API_KEY}"
        payload = {"contents": [{"parts": [{"text": prompt}]}], "generationConfig": {"response_mime_type": "application/json", "response_schema": json_schema}}
        response = requests.post(apiUrl, json=payload, timeout=120)
        response.raise_for_status()
        result = response.json()
        if not result.get('candidates'): raise Exception(f"API returned an unexpected response: {result}")
        data = json.loads(result['candidates'][0]['content']['parts'][0]['text'])
        return data.get('summary_paragraph', 'Summary generation failed.')
    except Exception as e:
        print(f"Gemini API Error: {e}")
        return "Failed to generate AI summary due to an API error."

def analyze_and_summarize_reviews(reviews, media_node):
    # --- GET MAL SCORE DIRECTLY ---
    mal_score = media_node.get('mean')
    final_score = f"{mal_score:.2f} / 10.00" if mal_score is not None else "N/A"

    full_text = ". ".join(reviews)
    all_sentences = [s.strip() for s in full_text.split('.') if s.strip()]
    dynamic_stop_themes = generate_dynamic_stop_themes(media_node)
    all_themes = extract_all_themes(full_text, nlp, dynamic_stop_themes)
    if not all_themes: return {"score": final_score, "summary": "Could not extract any themes to generate a summary."}
    
    top_themes = [theme for theme, count in Counter(all_themes).most_common(7)]
    theme_to_sentences = {
        theme: [s for s in all_sentences if any(word in s.lower() for word in theme.split())][:3]
        for theme in top_themes
    }
    
    # Pass the MAL score to Gemini for context
    summary = get_llm_summary({k: v for k, v in theme_to_sentences.items() if v}, final_score)
    
    return {"score": final_score, "summary": summary}

# ==============================================================================
# --- 3. FLASK API ENDPOINTS ---
# ==============================================================================
@app.route('/ping', methods=['GET'])
def ping():
    """A simple endpoint to check if the server is awake."""
    return jsonify({"status": "ok"})

@app.route('/search', methods=['GET'])
def search_media():
    title = request.args.get('title')
    if not title: return jsonify({"error": "A 'title' query parameter is required."}), 400
    results = get_media_from_mal(title)
    return jsonify(results)

@app.route('/analyze', methods=['POST'])
def analyze_media():
    data = request.json
    media_id = data.get('id')
    media_type = data.get('type')
    media_node = data.get('node')
    if not all([media_id, media_type, media_node]):
        return jsonify({"error": "Request body must contain 'id', 'type', and 'node'."}), 400
    slug = media_node['title'].replace(': ', '__').replace(' ', '_')
    reviews_base_url = f"https://myanimelist.net/{media_type}/{media_id}/{slug}/reviews"
    reviews = scrape_reviews_with_requests(reviews_base_url)
    if not reviews or 'error' in reviews:
        return jsonify({"error": "No reviews could be scraped for this title."})
    
    # Pass the media_node (which contains the MAL score) to the analysis function
    analysis_result = analyze_and_summarize_reviews(reviews, media_node)
    
    final_response = {
        'analysis': analysis_result,
        'media_info': {
            'title': media_node.get('title'),
            'english_title': media_node.get('alternative_titles', {}).get('en', ''),
            'image_url': media_node.get('main_picture', {}).get('large'),
            'synopsis': media_node.get('synopsis', 'No synopsis available.'),
            'genres': media_node.get('genres', [])
        }
    }
    return jsonify(final_response)

if __name__ == '__main__':
    # Use these settings for deployment on Hugging Face
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 7860)), debug=False)
